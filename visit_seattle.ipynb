{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Traverse all pages\n",
    "base_url = \"https://visitseattle.org/events/page/\"\n",
    "page = 1\n",
    "selector = \"div.search-result-preview > div > h3 > a\"\n",
    "events_link = []\n",
    "\n",
    "while True:\n",
    "    response = requests.get(base_url+str(page))\n",
    "    # Exit the loop if no next page is found\n",
    "    if response.status_code != 200:\n",
    "        break\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Scrape data \n",
    "    a_elements = soup.select(selector)\n",
    "    events_link += [x['href'] for x in a_elements]\n",
    "    \n",
    "    page += 1  # Increment to the next page\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the detail of event\n",
    "name_selector = \"div:nth-child(1) > div.medium-6.columns.event-top > h1\"\n",
    "date_selector = \"div:nth-child(1) > div.medium-6.columns.event-top > h4 > span:nth-child(1)\"\n",
    "location_selector = \"div:nth-child(1) > div.medium-6.columns.event-top > h4 > span:nth-child(2)\"\n",
    "type_selector = \"div:nth-child(1) > div.medium-6.columns.event-top > a:nth-child(3)\"\n",
    "region_selector = \"div:nth-child(1) > div.medium-6.columns.event-top > a:nth-child(4)\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for event in events_link:\n",
    "    res = requests.get(event)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    e_name = soup.select_one(name_selector).get_text().strip() if soup.select_one(name_selector) else 'Name Not Found'\n",
    "    e_date = soup.select_one(date_selector).get_text().strip() if soup.select_one(date_selector) else 'Date Not Found'\n",
    "    e_location = soup.select_one(location_selector).get_text().strip() if soup.select_one(location_selector) else 'Location Not Found'\n",
    "    e_type = soup.select_one(type_selector).get_text().strip() if soup.select_one(type_selector) else 'Type Not Found'\n",
    "    e_region = soup.select_one(region_selector).get_text().strip() if soup.select_one(region_selector) else 'Region Not Found'\n",
    "    data.append({\n",
    "        \"Name\": e_name, \n",
    "        \"Date\": e_date, \n",
    "        \"Location\": e_location,\n",
    "        \"Type\": e_type,\n",
    "        \"Region\": e_region\n",
    "    })\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "csv_file = 'events.csv'\n",
    "df.to_csv(csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_csv(input_file, output_file):\n",
    "    updated_rows = []\n",
    "    today = datetime.now().date()\n",
    "    \n",
    "    with open(input_file, mode='r') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        headers = next(reader)  # Store the header row\n",
    "        updated_rows.append(headers)\n",
    "\n",
    "        for row in reader:\n",
    "            # Skip rows with 'ongoing' in the 'Date' column, period events, and dates before today\n",
    "            if row[1].lower() == 'ongoing' or 'through' in row[1] or datetime.strptime(row[1], '%m/%d/%Y').date() < today:\n",
    "                continue\n",
    "\n",
    "            # Update the 'Region' value\n",
    "            if row[4] == 'Downtown':\n",
    "                row[4] = 'Seattle Downtown'\n",
    "            elif row[4] == 'South':\n",
    "                row[4] = 'South Seattle'\n",
    "            elif row[4] == 'North':\n",
    "                row[4] = 'North Seattle'\n",
    "            \n",
    "            updated_rows.append(row)\n",
    "\n",
    "    with open(output_file, mode='w', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerows(updated_rows)\n",
    "\n",
    "preprocess_csv('events.csv', 'preprocessed_events.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('47.5989361', '-122.3279543')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use OpenStreeetMap API to get (lat, lon) based on location name\n",
    "def get_lat_lon(location):\n",
    "    query_params = {\n",
    "        \"q\": location,\n",
    "        \"format\": \"jsonv2\"\n",
    "    }\n",
    "    response = requests.get(\"https://nominatim.openstreetmap.org/search.php\", params=query_params)\n",
    "    data = response.json()\n",
    "    if data:  # Check if data is not empty\n",
    "        return data[0]['lat'], data[0]['lon']\n",
    "    else:\n",
    "        print(f\"No data found for location: {location}\")\n",
    "        return None, None\n",
    "\n",
    "get_lat_lon(\"Seattle Downtown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chance of rain. Mostly cloudy, with a high near 58. East wind 3 to 9 mph. Chance of precipitation is 40%. New rainfall amounts less than a tenth of an inch possible.\n"
     ]
    }
   ],
   "source": [
    "# Loop up the weather\n",
    "def get_weather(lat, lon, date):\n",
    "    try:\n",
    "        # Convert date string to datetime object for comparison\n",
    "        target_date = datetime.strptime(date, '%m/%d/%Y')\n",
    "\n",
    "        # Get the API URL for the specific point\n",
    "        url = f\"https://api.weather.gov/points/{lat},{lon}\"\n",
    "        res = requests.get(url)\n",
    "        point_dict = res.json()\n",
    "\n",
    "        # Check if 'properties' key is present\n",
    "        if 'properties' in point_dict:\n",
    "            forecast_url = point_dict['properties']['forecast']\n",
    "            res = requests.get(forecast_url)\n",
    "            forecast_data = res.json()\n",
    "\n",
    "            # Search for the weather on the target date\n",
    "            if 'properties' in forecast_data and 'periods' in forecast_data['properties']:\n",
    "                for period in forecast_data['properties']['periods']:\n",
    "                    period_start = datetime.strptime(period['startTime'].split('T')[0], '%Y-%m-%d')\n",
    "                    if period_start.date() == target_date.date():\n",
    "                        return period['detailedForecast']  # Return the forecast data for the matching date\n",
    "            else:\n",
    "                print(\"Weather data not available for the specified date.\")\n",
    "        else:\n",
    "            print(\"Failed to retrieve forecast URL.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "lat, lon = get_lat_lon(\"Seattle Downtown\")\n",
    "weather_data = get_weather(lat, lon, \"1/30/2024\")\n",
    "print(weather_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the events CSV file\n",
    "with open('preprocessed_events.csv', mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    events = list(reader)\n",
    "\n",
    "# Add weather data to the events\n",
    "for event in events[1:]:  # Skip the header\n",
    "    lat, lon = get_lat_lon(event[4])\n",
    "    weather = get_weather(lat, lon, event[1])\n",
    "    event.append(weather)\n",
    "\n",
    "# Write the updated events to a new CSV file\n",
    "with open('updated_events.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(events)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
